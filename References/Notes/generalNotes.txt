--------------------------------------------------------------------------------
-------------------------------- Major Concepts --------------------------------
--------------------------------------------------------------------------------


===== useState()
syntax: const [name, setName] = React.useState(default);
               -a--  ----b---   ------c------- ---d---
    a. name of the variable that holds the state of the element (camelCase). Ex: elementState
    b. name of the method used to change that state. It's common convention to use "set" at the beginning to make it clear what this method does. Ex: setElementState
    c. this tells React that you are trying to manage state, and (I think) should only be used in the above syntax where a variable holds an array with two values (a. and b. defined above - the first being the state and the second being the state setter) and sets it to equal React.useState()
    d. the parameter passed into React.useState() to set the initial value of the state. Note that it can be an integer, boolean, array, string, etc. Ex: React.useState("initialValue")


===== useEffect()
syntax: useEffect( () => {}, [] );
                   ----a---- -b-
    a. the code block / instructions to be triggered
    b. the trigger to execute those instructions - it stores an array of states that, then it detects a change in state to any of the listed states (remember that state changes when using setState, defined when calling React.useState() ), it executes these commands. Reminder that the component who's state changed re-renders (basic functionality from states changing).
        - if nothing is passed in here, you can call useEffect without re-rendering anything and it will just trigger the instructions (defined in a.) once
        - if within the code block you change a variables state, that variable will re-render regardless of whether anything was passed into the second parameter of the function.
    - a great time to use useEffect() is when you don't want the action of getting data to cause a re-render, but rather upon successful fetching of data to display such data (an stopping once complete) - this is done by passing the empty array [] as the second parameter, signaling to useEffect() that this is to be carried out once and rerender should occur once complete. Passing nothing into the second parameter causes a lot of unnecessary background computation in rerendering.


===== useReducer() ===== (TODO: NEEDS REVIEW) ======
syntax: const [] = useReducer(reducerFunction, initialState)
        ---a----   ----b----  ------c--------  -----d------
    a.


===== async (asynchronous) function
syntax: const fetchData = async () = { const result = await axios(https://google.com); console.log(result) };
        -------a-------   ----b---     -----c------   --d-- --e-- --------f---------   ---------g---------
    a. the variable that will hold the function that will make the async call (can name anything you want). Later you can call fetchData() to call this async function
    b. defines that this is an async function (in this example without parameters)
    c. the variable that will hold the results of the async call
    d. defines the "promise", or the part that will be on standby


===== custom hook ===== (TODO: NEEDS REVIEW) ======
syntax (defining it): const useCustomHook = (parameter1, parameter2, etc) => {...};
                      --------a----------   -------------b--------------- ---c----
    a. variable later used as handle to invoke custom hook. NOTE: common convention has "use" at the beginning to easily point out it's a custom hook
    b. parameters/arguments for hook to use later in code block
    c. code block
syntax (using it): const [variableName, doSomething] = useCustomHook(arg1, arg2);
                   --a--  -----b------  -----c-----    ------d------ -----e-----
    a. just declares that what follows is a variable, part of the syntax (doesn't have to be const)
    b. like the useState() hook, defines a variable that will store state? Feels like useState... ===== (TODO: NEEDS REVIEW) ======
    c. function that changes state? Feels like useState... ===== (TODO: NEEDS REVIEW) ======
    d. calling the custom hook previously defined
    e. parameters passed into the custom hook (should match the number of arguments defined when the custom hook was defined)

--------------------------------------------------------------------------------
-------------------------------- Tips & Tricks ---------------------------------
--------------------------------------------------------------------------------

- Conditionally render an element:
syntax: { truthyOrFalseyVariable && <elementToRender />}
          -----------a---------- -b- --------c---------
    a. variable that holds a boolean value
    b. "And" operator
    c. element to be rendered
    This way, the element only renders if both sides of the operator are evaluated to true (simply defining an element evaluates to true)


--------------------------------------------------------------------------------
--------------------------- Other Terms to Recognize ---------------------------
--------------------------------------------------------------------------------
* Any commands to run in your console/terminal will be denoted using " " around the command. Run these without the " " *

- Running React program
    - to run a quick rendering of a React program, there are several things needed (to get it running locally, in a non-Prod ready state):
        - proper linking of files (to get one running real quick, check the /Notes/React/templates folder to see the required details here)
        - In your Terminal, download a basic http server (many can be used, but for this example I used http-server using npm) by running "npm install --global http-server" (installs it globally, so it doesn't matter where you are in your directory when you run it). Troubleshooting and details: https://www.npmjs.com/package/http-server
            - If you don't have npm, you can download it using "npm install -g npm" . Troubleshooting and details: https://docs.npmjs.com/downloading-and-installing-node-js-and-npm
        - Next, navigate to the folder containing the file you want to run, and run the command "http-server -c-1" (this runs it without caching, so you can change your code and refresh the browser in a later step to see your changes).
        - You'll notice something like this returned in the console if you did it right (just an example, might be different for you):
            Starting up http-server, serving ./

            http-server version: 14.1.1

            http-server settings: 
            CORS: disabled
            Cache: -1 seconds
            Connection Timeout: 120 seconds
            Directory Listings: visible
            AutoIndex: visible
            Serve GZIP Files: false
            Serve Brotli Files: false
            Default File Extension: none

            Available on:
            http://127.0.0.1:8080
            http://192.168.1.153:8080       <--- this is the port where your code is being displayed
            Hit CTRL-C to stop the server

        - above is marked for you the location of where you can view your code. Copy that line and paste it into a browser to see your code displaying. Remember you can pop open your browser's console to look for errors if you don't see anything displaying.

- Axios - library for fetching data
    - Axios vs. fetch()
        1. Axios is more browser compatible, but has to be imported/installed. If focusing on a wider user clientele (age, country, devices, etc), maybe you would want to use Axios for backward compatibility. It's more limited compared to Fetch() but needs less code to implement common commands (ex. it automatically formats JSON data)
        2. Fetch() is included with modern browsers and has more capabilities, but requires more code for common commands (you have to manually convert data into JSON for example). An example where this extra flexibility is useful is if you wanted to convert data fetched into .csv format for exporting.

- React Component Lifecycles (remove React element from page)
    - A component's lifecycle begins when it's mounted to the DOM.  This happens only once. It can then be updated in the DOM as many times as necessary. Finally, it is unmounted from the DOM when it's no longer displayed on the screen. In the example below, you’ll see a basic React component that goes through this lifecycle:
        ex. let someComponent = React.createElement(someComponent)
    - To render this React component, aka mount it to the DOM, you can write the following:
        ex. ReactDOM.render(
                someComponent, document.getElementById("root")
            );
    - Suppose, you want to remove the component from the DOM after it completes an operation. You can use the React API unmountComponentAtNode()
        ex. handleClick() {
                unmountComponentAtNode(document.getElementById('root'));
            };
- Prop-drilling - term used in React when a Parent component is passing down data (properties, or props) to child elements

- React Context - a way of prop-drilling through many react components a state (or otherwise communicating some change) by making it global so all components can know that state (or boolean or whatever)
    - since it complicates how the application runs and manages this info, it's not needed in more simple applications (parent -> child), but moreso when many nested components need to know the state (or w.e.) of a Parent component (parent -> children -> children -> children)

- Strapi - back-end database service, installed locally

- Postman - application (downloaded onto the computer so it can interact with localhost, otherwise can be viewed in the browser) that can test databases by making POST and GET requests (and can be followed up by visiting Strapi to see if the POST went through, or stay in Postman for GETs to see the response)
    - NOTE: at the right hand of the application, look for a "code" button. This is super useful to show you how to make the selected POST or GET request across different languages!!!

- Static Website - a site that, once loaded, does not communicate with a server or database to further function. It is self-contained, and thus makes it hard to hack (or allow hackers to steal information).

- GraphQL - a query language for APIs that helps manage POSTs and uses a query in which we specify exactly what data we want in the response.
    - it is database agnostic, so it can be used with any top database option

- Mock - adding a fake dependency to a test (we did it in Jest) to work around real dependencies that the app the test is running against would have.

- Routes - paths that define an API (think Bad Bank application, where routes were used to define different pages to load into the main page)

- Node - an open-source, cross-platform runtime environment that allows developers to create all kinds of server-side tools and applications in JavaScript. Think NPM (node package manager)
    - to use Node in your application (like separate from npm, which just uses node to manage packages) you have to first initialize node using "node init"
        - there will be questions asked to get the details of the server you're about to spin up, make logical choices here but I don't think it impacts too much
    - you can call "node [filename]" to run that file with the server you spun up
    - NOTE: remember that when trying to run a Node server, you need to download dependencies AND initialize the data store
        - dependencies in class: Express ("npm install express"), Lowdb ("npm install lowdb") - NOTE: if already in your package.json file, "npm install" will already know what (and which version) to download
        - data store in class: eun "npm init" to set up, entering more or less default names for the values asked
            - once done, you can run "cat packagae.json" to see the metadata file created with details

- Express - the most popular Node web framework (unopinionated). It can:
    - write handlers for requests with different HTTP verbs at different URL paths (routes)
    - integrate with "view" rendering engines in order to generate responses by inserting data into templates
    - set common web application settings like the port to use for connecting, and the location of templates that are used for rendering the response
    - add additional request processing "middleware" at any point within the request handling pipeline

- Opinionated VS Unopinionated frameworks:
    - Opinionated: "there is a right way to solve this problem"
        - less flexible in tools or options to resolve problems
        - well-documented and reliable solutions for given issues
        - narrower scope
    - Unopinionated: "get creative to find a solution"
        - very flexible, incorporates outside components into workflow
        - easier for developers to navigate (no strict "do A then B then C" to resolve an issue)
        - no good direction to resolve things - it's up to you to find a solution that works

- URL Routes - when working locally, the URL denotes imformation hierarchy (how it is being accessed). This will be made clearer with an example:
    - Let's say you're looking at a (GET in this case) request from an app trying to display a post on a blog - the code can look something like this:
        - ex: app.get('/posts/:title/:id/:published', function(req, res){...
                      -------relevant bit here------
    You can see that the app is looking in the 'posts' part of the database, and the further parts are just parameters being passed in
    Let's look at what the URL looks like once the information is returned:
        - ex: http://localhost:3000/post/Hello/91991/false
                    -----a------  -b-  --c-- --d-- --e--
                                        -------i---------
            a. which port you are currently connected to
            b. Route - the higher up umbrella group. In this case, we're looking at a post (like blog post)
            i. the rest are just parameters to the data being accessed, parameters to the post data structure
                that specify what within posts we're looking at
            c. title - nested in the post category is a specific post called Hello
            d. id - some very specific element within that post (or even the id of the post)
            e. boolean - some true or false value used to keep track of whether it's published

- Mongo DB - database service used for this class. For class I installed the comunity version (I don't think I had an option), version 6.
    Running/Stopping as a macOS service:
    - To run:  brew services start mongodb-community@6.0
    - To stop running:  brew services stop mongodb-community@6.0
    Running manually as a backgrgound process:
    - (Intel Chip mac):  mongod --config /usr/local/etc/mongod.conf --fork
    - (M1 chip):  mongod --config /opt/homebrew/etc/mongod.conf --fork
    * To stop running as a background process:
    - connect to "mongod" using "mongosh", run the "shutdown" command (google these terms to know more)
    Notes taken from here: https://www.mongodb.com/docs/manual/tutorial/install-mongodb-on-os-x/

- Docker - a service of creating copies, or images, of your code into a docker server to have it as a neat package, or container, so others can run your code on the docker servers without needoing to download dependencies and such locally.
    To "dockerize" your project (first explaining the set up, then after I'll explain the actually creating of the container), download the Docker desktop app first at https://www.docker.com/products/docker-desktop/ <- it'll make this a lot easier:

    - create a "docker-compose.yml" file in the root directory of the project. This will give instructions to docker commands you will run later on the structure of the project.
        --- Example docker-compose.yml contents for non-nested project (super basic):
            version: "3"

            services:
            node:
            build: .
            ports:
            - "3000:3000"
            command: node index.js


            You're telling docker that when you run the docker command (that will create an image of your project), use version 3, use ports 3000 (I think that's on the mdocker machine), and to run the code use the command "node index.js" to get it started
                NOTE: in this simple example all the files are in the same directory so they are all accessible without further navigation

        
        --- Example docker-compose.yml contents for multi-tiered application:
            version: '2'
            services:
            client:
                build: './client'
                ports:
                - '3000:3000'
                depends_on:
                - server
            server:
                build: './server'
                ports:
                - '8080:8080'
                depends_on:
                - mongo
            mongo:
                image: mongo
                ports:
                - '27017:27017'


            You're telling docker that when you run the docker command (that will create an image of your project):
                - user version 2
                - there is a client part to this app
                    - it's in the client folder that's in the same directory as where you're running the docker build command from
                    - use port 3000 for this client app
                    - it relies on the server part of the app, defined next
                - there is a server part to this app
                    - it's in the server folder also in the same directory
                    - use prt 8080 (since 3000 is taken from running the Client code)
                    - relies on Mongo (database)
                - there's a database (Mongo) part to this app
                    - look online for the publicly available Mongo image and use that
                    - use port 27017 for this functionality


    - create further instructions to upload the image into a Docker container by creating a "Dockerfile" (notice it doesn't have a file-type like .js or something):
        --- Example Dockerfile contents:
            FROM node:slim

            LABEL oscar otenorio90@gmail.com

            WORKDIR /app

            COPY index.js /app/index.js
            COPY package.json /app/package.json
            RUN npm install
            EXPOSE 3000
            ENTRYPOINT node index.js


            You're telling the docker command (which I haven't given yet, this is still part of setup):
                - use a starting template of a node app (slim version so it's smaller)
                - make a LABEL that will note who made/maintains this code
                - create a folder on the docker machine -side called app (where the files will go)
                - grab the contents of the index.js file (note syntax since it's in the same directory as this Dockerfile) and place them into an index.js file you'll create in the app directory
                    NOTE: you can alos just use "COPY . ." to copy all the files in the current directory into a copy in the docker directory
                - do the same for package.json
                - run "npm install" (uses the contents of package.json to download dependencies on the docker machine)
                - open up port 3000
                - use the command "node index.js" to run this code
            NOTE: this applies to any complexity of the app (simple or multi-tiered) but the convention seems to be that you make a Dockerfile for each tier.
                Example:
                √  project-directory
                    √  Client-code
                        >  node_modules
                        >  img
                        √  folder-containing-source-files
                        package.json
                        .gitignore
                        Dockerfile  <-----
                    √  Server-code
                        >  node_modules
                        index.js
                        package.json
                        .gitignore
                        Dockerfile  <-----
                    docker-compose.yml   <----------- NOTE: the yml file should live in the project root where you call the docker build command - it'll go looking into child directories for the Dockerfiles


    - FINALLY actually run the docker build command to dockerize this puppy:
        Run this command from the root directory of the project (where the docker-compose.yml file is):
        - first time running the command use:  docker-compose up
            - creates the image into a container wil default naming conventions based on folder arcchitecture (recommended)
        - following times use:  docker-compose build
            - overwrites the image using the same naming convention (recommended to overwrite your other work). I think "docker-compose up" will fail if it already exists
        


    Docker hub is like the github of docker containers, where you can share your work and others can run it without downloading a bunch of stuff on their machines.

    To add to docker hub:
    - NOTE: first you should go to https://hub.docker.com and create a file. Created my username as "oscartenorio", so wherever you see that in further notes just know that's your username
    - Navigate to root of project that you want to put up into docker hub
    - Run:
        docker build -t oscartenorio/project_name .
        -----a------ -b -----c-----  -----d------ -e
        a. command to create the docker container
        b. adds a tag
        c. corresponds to your docker hub username (must make an account first for this to work)
        d. name of the project (it'll be in the url later)
        e. not sure, but I think it runs the application once built or something?
    - this will upload an image of your project to docker hub, where you can send the URL to anyone (I gugess depending on privacy/permission settings) and have them run your code

    To test that it worked:
        - Run (from the same place in the directory as above, I think):
            docker run -p 8080:3000 --name project_nameInstance -t oscartenorio/project_name
            -----a---- -b -c-- --d- ---e-- --------f----------  ------------g---------------
            a. run command
            b. port tag
            c. port to expose on your machine (D1 local)
            d. port inside of the container
            e. name tag
            f. the name of the container / name of the instanced image (notice the naming convention, so like "exampleInstance" or "myProjectInstance")
            g. destination to run from (same that we created above)
        - To stop it from running:
            - run  docker ps -a  to get a list of the running processes in a NEW terminal, and grab the id of the running container
            - run  docker stop id_you_grabbed  to terminate the process


    To push / pull from Docker hub, you need to log in first:
        docker login -u "myusername" -p "mypassword" docker.io
        - where "myusername" and "mypassword" are replaced with actual username and password (no quotes)

    To push to Docker hub:
        - Run:
            docker push oscartenorio/project_name

    To pull from Docker hub:
    - Run:
        docker pull oscartenorio/project_name

    To remove the container/images:
        - from local machine:
            docker ps -a
            - grab id of the local image
            docker rm id_you_grabbed
        - from docker hub
            docker images
            - grab id of the docker container
            docker rmi id_you_grabbed


--------------------------------------------------------------------------------
----------------------------- Copied from Class --------------------------------
--------------------------------------------------------------------------------

- REST (REpresentational State Transfer): copied from a lecture slide:
    Representational state transfer (REST) is a standard for communication between an application’s server and its client. It is considered the industry-standard protocol for web APIs, so you will likely answer questions about REST during software development interviews. It’s important to understand that REST is not a technology or framework; it’s an architectural style to support any API design. 

    RESTful APIs adhere to these five principles.
    1.Contract-first approach / Uniform Resource Identifiers

    The “contract” in the Contract-first approach refers to a contract between your client application and the server. The client app needs to know that it can call the API endpoint and get the data in the expected format each time.

    Contract Example:

    Let’s suppose you had a server with the endpoint “/api/v1/orders/14829/status” that returned the status of order number `14829.` As a RESTful API, the client would expect that:

        The API endpoint does not change
        The data format does not change
        The API documentation describes what information is stored and how

    2. Statelessness

    The client application should assume that the server did not preserve the state from previous API calls. The client application should provide all the information necessary for the server to respond to each call. What does this mean? 

    Say you (the client application) asked your neighbor (the server application) about a mutual friend. In a real-life conversation, the following questions would be stateful:

    How old is Frank Gibson? What does he do for work?

    You (the client application) expect your neighbor (the server application) to remember the mutual friend’s name. So, the second question assumes that the question is about Frank Gibson.

    In a RESTful API, the same conversation between the client and server applications would instead go like this:

    How old is Frank Gibson?

    What does Frank Gibson do for work?

    The client provides all the information a server needs to respond to every API call, assuming that the server remembers nothing of the previous API calls.
    3. Client-server Model

    The client application doesn’t care about how the server stores the data or the technology used. The only thing it cares about is the database schema which contains a description of the stored data and the layout in which it is stored.
    4. Caching

    Caching is the temporary storage of information outside the server. The client can cache data locally to have it available without making an additional API call. 
    5. Layered architecture

    When you have a client application that calls an API endpoint that triggers logic on your back-end server application, you have a layered architecture. What’s important in REST is that each layer only knows about the layer next to it and no more. This is also called a separation-of-concerns. Each layer has a specific job and passes information to and from its neighboring layers to get the job done. Thus, the only layer that the client application knows about is the API, which is its immediate neighbor; it does not know about any other layers beyond the API.

----------------------------------------------------------------
- GraphQL vs REST

    In a standard RESTful API, you typically hit an endpoint that determines what data is returned.GraphQL improves upon the REST philosophy by allowing the front-end application to ask for a specific piece of that data.

    Let's say you want to display the logged-in user's email address on their profile page.

    To do this with a REST API, you would:

        Call an“auth/user” endpoint and get all of the user's information. 
        Pull the email address out of that returned data 
        Display the email address

    Issues with REST:

        If the logged-in user has many data associated with it, such as a list of events, order history, addresses, and phone numbers, it will all be returned even if you don’t need this information. 
        Unnecessary information is passed around. 

    GraphQL allows you to query only the data you want. If you replace the REST endpoint with a GraphQL one, you could query the currently logged-in user but specify you only want their email address. 

----------------------------------------------------------------
- Common packages to hekp a developer in the real world:

    Throughout this course, you’ve seen many packages that you can use to build impressive features and applications. Now let’s look at some that can make your life as a developer easier:

        - Faker.js
        https://www.npmjs.com/package/@faker-js/faker
        As you’ve seen in previous videos, Faker.js can help you generate dummy front-end and back-end data to test your application. You can create people, organizations, accounts, products, images, and more. You can use the same data that are used in production for simplicity and speed. And Faker.js will always keep your data safe.

        - Chalk
        https://www.npmjs.com/package/chalk
        You often check console.log when debugging. However, back-end logs can blend in the terminal and become difficult to identify. With Chalk, you can customize your console.log so that particular errors appear bright red, with a background color, or underlined.

        - Moment
        https://www.npmjs.com/package/moment
        Comparing or manipulating dates or timestamps can be confusing, especially when you have to consider time differences between the user and the server. Moment has been the standard and most widely used date library for a long time, and for a good reason. For instance, if you want to know if one date is older than another, you simply have to write: (png that I can't here) Or maybe you want to add one month to an order creation date to see when it needs to be paid (another png I can't add here).

        Moment does a lot more, and you’re encouraged to take a look at its documentation (https://momentjs.com/) to get a good sense of all you can do with it.

        Note: As of September 2020, Moment recommends using a different date library when building new applications. Nevertheless, with over 12 million downloads per week, Moment is still the most used date library. Therefore, it’s important that you become familiar with its basic functionality and capabilities. For more information and other libraries recommended by Moment, take a look at its Project Status (https://momentjs.com/docs/)

----------------------------------------------------------------
- What Is Heroku?

    https://www.heroku.com/
    Heroku is a cloud service platform that allows you to deploy and run your apps. This platform offers support for a wide range of programming languages, such as Java, Ruby, Node.js, and Python. Heroku runs applications in virtual containers known as Dynos. Think of Dynos as virtual machines that run your applications on the cloud.

    Heroku has a free tier to help you get started. You can create a Heroku account and then follow this getting started guide
    Links to an external site. to start using Heroku services. This will help you accomplish some of the deployment tasks required throughout the course.

----------------------------------------------------------------
- Docker Convenient Notes:
    Useful Docker Commands:

    Build image
    # The -t gives your image a name
    $ docker build -t YourImageName.

    List Containers
    # Containers currently running
    $ docker ps -a

    List Images
    # List all Docker images
    $ docker images -a

    Delete containers
    # Delete every Docker container
    # Run this command first because images are attached to containers
    $ docker rm $(docker ps -a -q)

    Delete images
    # Delete every Docker image
    $ docker rmi $(docker images -q)

    Force delete images
    # to force delete images to prevent their use
    $ docker rmi -f $(docker images -q)

    Stop container
    # Stop a container
    $ docker stop <container id>

    Shell in running container
    # From Docker 1.3 onwards
    $ sudo docker exec -i -t <containerIdOrName> bash
    --------------------------------
    Create and Publish A Docker Image:
    Create an application and a docker file for the application. Then, create a docker image for the application. Name your image YourDockerUsername/YourImageName. In this example the username is "abelsan" and the image name is "cabinfever".

    # Create Docker image
    $ docker build -t abelsan/cabinfever.

    Test your new image.

    # "-p 8080:3000" sets the container port to 3000 and the host port to 8080
    # "--name cabinfeverInstance" sets the container name to cabinfeverInstance
    # "-t cabinfever" references the cabinfever image
    $ docker run -p 8080:3000 --name cabinfeverInstance -t abelsan/cabinfever

    After you confirm your application works, push your image to docker hub.

    # Note the username/image matches the Docker repository
    $ docker push abelsan/cabinfever

    To use the image, enter the pull command.

    $ docker pull abelsan/cabinfever
    --------------------------------
    Docker - NodeJS Hello World:
    Use this document as a way to quickly start any new project.

    Create a barebones NodeJS application
    Create a directory named docnode. Initialize your application (npm init). Install express (npm install express --save). Your package.json file should look like the following:

    {
    "name": "docnode",
    "version": "1.0.0",
    "description": "barebones node on docker",
    "main": "index.js",
    "scripts": {
        "test": "echo \"Error: no test specified\" && exit 1"
    },
    "author": "abel@mit.edu",
    "license": "MIT",
    "dependencies": {
        "express": "^4.15.5"
    }
    }

    Add a bare-bones express server and name your file index.js:
    var express = require('express');
    var app     = express();
    
    app.get('/', function(req,res){
    res.send('Hello World!');
    });
    
    var port = 3000;
    app.listen(port,function(){
    console.log('Listening on port:' + port);
    });

    Create a Docker file and name it Dockerfile:
    FROM node:slim
    MAINTAINER abelsan <abel@mit.edu>
    WORKDIR /app
    # copy code, install npm dependencies
    COPY index.js /app/index.js
    COPY package.json /app/package.json
    RUN npm install

    Create a Docker compose file and name it docker-compose.yml:
    version: "2"
    services:
    gin:
        build: .
        ports:
        - "3000:3000"

    If needed, copy your code to the remote machine. After this is done, at the command line, enter docker-compose up:

    docker-compose up
    --------------------------------
    How To Install Docker On Ubuntu:
    Step 1 — Installing Docker
    The Docker installation package available in the official Ubuntu 16.04 repository may not be the latest version. To get the latest version, install Docker from the official Docker repository. This section shows you how to do that.

    First, add the GPG key for the official Docker repository to the system:

    $ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -

    Add the Docker repository to APT sources:

    $ sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"

    Next, update the package database with the Docker packages from the newly added repository:

    $ sudo apt-get update

    Ensure that you install from the Docker repo instead of the default Ubuntu 16.04 repo:

    apt-cache policy docker-ce

    You should see output similar to the following:

    Output of apt-cache policy docker-ce

    docker-ce:

    Installed: (none)

    Candidate: 17.03.1~ce-0~ubuntu-xenial

    Version table:

    17.03.1~ce-0~ubuntu-xenial 500

    500 https://download.docker.com/linux/ubuntu xenial/stable amd64 Packages

    17.03.0~ce-0~ubuntu-xenial 500

    500 https://download.docker.com/linux/ubuntu xenial/stable amd64 Packages

    Notice that docker-ce is not installed, but the candidate for installation is from the Docker repository for Ubuntu 16.04. Also note that the docker-ce version number might be different.

    Finally, install Docker:

    $ sudo apt-get install -y docker-ce

    Docker should now be installed, the daemon started, and the process enabled to start on boot. Check that it's running:

    $ sudo systemctl status docker

    The output should be similar to the following, showing that the service is active and running:

    Output

    docker.service - Docker Application Container Engine
    Loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset: enabled)

    Active: active (running) since Sun 2016-05-01 06:53:52 CDT; 1 weeks 3 days ago

    Docs: https://docs.docker.com

    Main PID: 749 (docker)

    

    The installation of Docker gives you not just the Docker service (daemon) but also the Docker command line utility or the Docker client. We'll explore how to use the Docker command later in this tutorial.

    Step 2 — Executing The Docker Command Without Sudo (Optional)
    By default, running the Docker command requires root privileges; therefore, you have to prefix the command with sudo. It can also be run by a user in the Docker group, which is automatically created during the Docker Installation. If you attempt to run the Docker command without prefixing it with sudo or without being in the docker group, you'll get an output like this:

    Output

    docker: Cannot connect to the Docker daemon. Is the docker daemon running on this host?.

    See 'docker run --help'.

    If you want to avoid typing sudo whenever you run the Docker command, add your username to the Docker group:

    $ sudo usermod -aG docker ${USER}

    To apply the new group membership, log out of the server, then log back in.
    ----------------------------------------------------------------


--------------------------------------------------------------------------------
------------------------------- Useful Links -----------------------------------
--------------------------------------------------------------------------------

- https://reactjs.org/ - React JS documentation and resources

- https://getbootstrap.com - Bootstrap CSS documentation and resources

- https://strapi.io/ - Strapi (back-end database) documentation and resources

- https://www.npmjs.com/ - npm package registry (look up commands to install specific packages)
