--------------------------------------------------------------------------------
-------------------------------- Major Concepts --------------------------------
--------------------------------------------------------------------------------


===== useState()
syntax: const [name, setName] = React.useState(default);
               -a--  ----b---   ------c------- ---d---
    a. name of the variable that holds the state of the element (camelCase). Ex: elementState
    b. name of the method used to change that state. It's common convention to use "set" at the beginning to make it clear what this method does. Ex: setElementState
    c. this tells React that you are trying to manage state, and (I think) should only be used in the above syntax where a variable holds an array with two values (a. and b. defined above - the first being the state and the second being the state setter) and sets it to equal React.useState()
    d. the parameter passed into React.useState() to set the initial value of the state. Note that it can be an integer, boolean, array, string, etc. Ex: React.useState("initialValue")


===== useEffect()
syntax: useEffect( () => {}, [] );
                   ----a---- -b-
    a. the code block / instructions to be triggered
    b. the trigger to execute those instructions - it stores an array of states that, then it detects a change in state to any of the listed states (remember that state changes when using setState, defined when calling React.useState() ), it executes these commands. Reminder that the component who's state changed re-renders (basic functionality from states changing).
        - if nothing is passed in here, you can call useEffect without re-rendering anything and it will just trigger the instructions (defined in a.) once
        - if within the code block you change a variables state, that variable will re-render regardless of whether anything was passed into the second parameter of the function.
    - a great time to use useEffect() is when you don't want the action of getting data to cause a re-render, but rather upon successful fetching of data to display such data (an stopping once complete) - this is done by passing the empty array [] as the second parameter, signaling to useEffect() that this is to be carried out once and rerender should occur once complete. Passing nothing into the second parameter causes a lot of unnecessary background computation in rerendering.


===== useReducer() ===== (TODO: NEEDS REVIEW) ======
syntax: const [] = useReducer(reducerFunction, initialState)
        ---a----   ----b----  ------c--------  -----d------
    a.


===== async (asynchronous) function
syntax: const fetchData = async () = { const result = await axios(https://google.com); console.log(result) };
        -------a-------   ----b---     -----c------   --d-- --e-- --------f---------   ---------g---------
    a. the variable that will hold the function that will make the async call (can name anything you want). Later you can call fetchData() to call this async function
    b. defines that this is an async function (in this example without parameters)
    c. the variable that will hold the results of the async call
    d. defines the "promise", or the part that will be on standby


===== custom hook ===== (TODO: NEEDS REVIEW) ======
syntax (defining it): const useCustomHook = (parameter1, parameter2, etc) => {...};
                      --------a----------   -------------b--------------- ---c----
    a. variable later used as handle to invoke custom hook. NOTE: common convention has "use" at the beginning to easily point out it's a custom hook
    b. parameters/arguments for hook to use later in code block
    c. code block
syntax (using it): const [variableName, doSomething] = useCustomHook(arg1, arg2);
                   --a--  -----b------  -----c-----    ------d------ -----e-----
    a. just declares that what follows is a variable, part of the syntax (doesn't have to be const)
    b. like the useState() hook, defines a variable that will store state? Feels like useState... ===== (TODO: NEEDS REVIEW) ======
    c. function that changes state? Feels like useState... ===== (TODO: NEEDS REVIEW) ======
    d. calling the custom hook previously defined
    e. parameters passed into the custom hook (should match the number of arguments defined when the custom hook was defined)

--------------------------------------------------------------------------------
-------------------------------- Tips & Tricks ---------------------------------
--------------------------------------------------------------------------------

- Conditionally render an element:
syntax: { truthyOrFalseyVariable && <elementToRender />}
          -----------a---------- -b- --------c---------
    a. variable that holds a boolean value
    b. "And" operator
    c. element to be rendered
    This way, the element only renders if both sides of the operator are evaluated to true (simply defining an element evaluates to true)
    
- For making CRUD database insertions using GraphQL (so importnat for the Bad Bank exercise), see the following file:
    /Users/otenorio/Desktop/MIT_MERN_fullstack/oscartenorio.github.io/References/Notes/Module_24_express_server_and_graphQL/expressgraphQL01/index.js line 75
    NOTE: where exactly the file is located will be dependant on where you're viewing this from, but just following tthe above path from oscartenorio.github.io on should do the trick
    - you can see it done for restaurants at the following as well: /Users/otenorio/Desktop/MIT_MERN_fullstack/oscartenorio.github.io/References/Notes/Module_24_express_server_and_graphQL/graphql-restaurant-data-exercise_starter/index.js


--------------------------------------------------------------------------------
--------------------------- Other Terms to Recognize ---------------------------
--------------------------------------------------------------------------------
* Any commands to run in your console/terminal will be denoted using " " around the command. Run these without the " " *

- Running React program
    - to run a quick rendering of a React program, there are several things needed (to get it running locally, in a non-Prod ready state):
        - proper linking of files (to get one running real quick, check the /Notes/React/templates folder to see the required details here)
        - In your Terminal, download a basic http server (many can be used, but for this example I used http-server using npm) by running "npm install --global http-server" (installs it globally, so it doesn't matter where you are in your directory when you run it). Troubleshooting and details: https://www.npmjs.com/package/http-server
            - If you don't have npm, you can download it using "npm install -g npm" . Troubleshooting and details: https://docs.npmjs.com/downloading-and-installing-node-js-and-npm
        - Next, navigate to the folder containing the file you want to run, and run the command "http-server -c-1" (this runs it without caching, so you can change your code and refresh the browser in a later step to see your changes).
        - You'll notice something like this returned in the console if you did it right (just an example, might be different for you):
            Starting up http-server, serving ./

            http-server version: 14.1.1

            http-server settings: 
            CORS: disabled
            Cache: -1 seconds
            Connection Timeout: 120 seconds
            Directory Listings: visible
            AutoIndex: visible
            Serve GZIP Files: false
            Serve Brotli Files: false
            Default File Extension: none

            Available on:
            http://127.0.0.1:8080
            http://192.168.1.153:8080       <--- this is the port where your code is being displayed
            Hit CTRL-C to stop the server

        - above is marked for you the location of where you can view your code. Copy that line and paste it into a browser to see your code displaying. Remember you can pop open your browser's console to look for errors if you don't see anything displaying.

- Axios - library for fetching data
    - Axios vs. fetch()
        1. Axios is more browser compatible, but has to be imported/installed. If focusing on a wider user clientele (age, country, devices, etc), maybe you would want to use Axios for backward compatibility. It's more limited compared to Fetch() but needs less code to implement common commands (ex. it automatically formats JSON data)
        2. Fetch() is included with modern browsers and has more capabilities, but requires more code for common commands (you have to manually convert data into JSON for example). An example where this extra flexibility is useful is if you wanted to convert data fetched into .csv format for exporting.

- React Component Lifecycles (remove React element from page)
    - A component's lifecycle begins when it's mounted to the DOM.  This happens only once. It can then be updated in the DOM as many times as necessary. Finally, it is unmounted from the DOM when it's no longer displayed on the screen. In the example below, you’ll see a basic React component that goes through this lifecycle:
        ex. let someComponent = React.createElement(someComponent)
    - To render this React component, aka mount it to the DOM, you can write the following:
        ex. ReactDOM.render(
                someComponent, document.getElementById("root")
            );
    - Suppose, you want to remove the component from the DOM after it completes an operation. You can use the React API unmountComponentAtNode()
        ex. handleClick() {
                unmountComponentAtNode(document.getElementById('root'));
            };
- Prop-drilling - term used in React when a Parent component is passing down data (properties, or props) to child elements

- React Context - a way of prop-drilling through many react components a state (or otherwise communicating some change) by making it global so all components can know that state (or boolean or whatever)
    - since it complicates how the application runs and manages this info, it's not needed in more simple applications (parent -> child), but moreso when many nested components need to know the state (or w.e.) of a Parent component (parent -> children -> children -> children)

- Strapi - back-end database service, installed locally

- Postman - application (downloaded onto the computer so it can interact with localhost, otherwise can be viewed in the browser) that can test databases by making POST and GET requests (and can be followed up by visiting Strapi to see if the POST went through, or stay in Postman for GETs to see the response)
    - NOTE: at the right hand of the application, look for a "code" button. This is super useful to show you how to make the selected POST or GET request across different languages!!!

- Static Website - a site that, once loaded, does not communicate with a server or database to further function. It is self-contained, and thus makes it hard to hack (or allow hackers to steal information).

- GraphQL - a query language for APIs that helps manage POSTs and uses a query in which we specify exactly what data we want in the response.
    - it is database agnostic, so it can be used with any top database option

- Mock - adding a fake dependency to a test (we did it in Jest) to work around real dependencies that the app the test is running against would have.

- Routes - paths that define an API (think Bad Bank application, where routes were used to define different pages to load into the main page)

- Node - an open-source, cross-platform runtime environment that allows developers to create all kinds of server-side tools and applications in JavaScript. Think NPM (node package manager)
    - to use Node in your application (like separate from npm, which just uses node to manage packages) you have to first initialize node using "node init"
        - there will be questions asked to get the details of the server you're about to spin up, make logical choices here but I don't think it impacts too much
    - you can call "node [filename]" to run that file with the server you spun up
    - NOTE: remember that when trying to run a Node server, you need to download dependencies AND initialize the data store
        - dependencies in class: Express ("npm install express"), Lowdb ("npm install lowdb") - NOTE: if already in your package.json file, "npm install" will already know what (and which version) to download
        - data store in class: eun "npm init" to set up, entering more or less default names for the values asked
            - once done, you can run "cat packagae.json" to see the metadata file created with details

- Express - the most popular Node web framework (unopinionated). It can:
    - write handlers for requests with different HTTP verbs at different URL paths (routes)
    - integrate with "view" rendering engines in order to generate responses by inserting data into templates
    - set common web application settings like the port to use for connecting, and the location of templates that are used for rendering the response
    - add additional request processing "middleware" at any point within the request handling pipeline

- Opinionated VS Unopinionated frameworks:
    - Opinionated: "there is a right way to solve this problem"
        - less flexible in tools or options to resolve problems
        - well-documented and reliable solutions for given issues
        - narrower scope
    - Unopinionated: "get creative to find a solution"
        - very flexible, incorporates outside components into workflow
        - easier for developers to navigate (no strict "do A then B then C" to resolve an issue)
        - no good direction to resolve things - it's up to you to find a solution that works

- URL Routes - when working locally, the URL denotes imformation hierarchy (how it is being accessed). This will be made clearer with an example:
    - Let's say you're looking at a (GET in this case) request from an app trying to display a post on a blog - the code can look something like this:
        - ex: app.get('/posts/:title/:id/:published', function(req, res){...
                      -------relevant bit here------
    You can see that the app is looking in the 'posts' part of the database, and the further parts are just parameters being passed in
    Let's look at what the URL looks like once the information is returned:
        - ex: http://localhost:3000/post/Hello/91991/false
                      -----a------  -b-  --c-- --d-- --e--
                                        -------i---------
            a. which port you are currently connected to
            b. Route - the higher up umbrella group. In this case, we're looking at a post (like blog post)
            i. the rest are just parameters to the data being accessed, parameters to the post data structure
                that specify what within posts we're looking at
            c. title - nested in the post category is a specific post called Hello
            d. id - some very specific element within that post (or even the id of the post)
            e. boolean - some true or false value used to keep track of whether it's published

- Mongo DB - database service used for this class. For class I installed the comunity version (I don't think I had an option), version 6.
    Running/Stopping as a macOS service:
    - To run:  brew services start mongodb-community@6.0
    - To stop running:  brew services stop mongodb-community@6.0
    Running manually as a backgrgound process:
    - (Intel Chip mac):  mongod --config /usr/local/etc/mongod.conf --fork
    - (M1 chip):  mongod --config /opt/homebrew/etc/mongod.conf --fork
    * To stop running as a background process:
    - connect to "mongod" using "mongosh", run the "shutdown" command (google these terms to know more)
    Notes taken from here: https://www.mongodb.com/docs/manual/tutorial/install-mongodb-on-os-x/

- Docker - a service of creating copies, or images, of your code into a docker server to have it as a neat package, or container, so others can run your code on the docker servers without needoing to download dependencies and such locally.
    To "dockerize" your project (first explaining the set up, then after I'll explain the actually creating of the container), download the Docker desktop app first at https://www.docker.com/products/docker-desktop/ <- it'll make this a lot easier:

    - create a "docker-compose.yml" file in the root directory of the project. This will give instructions to docker commands you will run later on the structure of the project.
        --- Example docker-compose.yml contents for non-nested project (super basic):
            version: "3"

            services:
            node:
            build: .
            ports:
            - "3000:3000"
            command: node index.js


            You're telling docker that when you run the docker command (that will create an image of your project), use version 3, use ports 3000 (I think that's on the mdocker machine), and to run the code use the command "node index.js" to get it started
                NOTE: in this simple example all the files are in the same directory so they are all accessible without further navigation

        
        --- Example docker-compose.yml contents for multi-tiered application:
            version: '2'
            services:
            client:
                build: './client'
                ports:
                - '3000:3000'
                depends_on:
                - server
            server:
                build: './server'
                ports:
                - '8080:8080'
                depends_on:
                - mongo
            mongo:
                image: mongo
                ports:
                - '27017:27017'


            You're telling docker that when you run the docker command (that will create an image of your project):
                - user version 2
                - there is a client part to this app
                    - it's in the client folder that's in the same directory as where you're running the docker build command from
                    - use port 3000 for this client app
                    - it relies on the server part of the app, defined next
                - there is a server part to this app
                    - it's in the server folder also in the same directory
                    - use prt 8080 (since 3000 is taken from running the Client code)
                    - relies on Mongo (database)
                - there's a database (Mongo) part to this app
                    - look online for the publicly available Mongo image and use that
                    - use port 27017 for this functionality


    - create further instructions to upload the image into a Docker container by creating a "Dockerfile" (notice it doesn't have a file-type like .js or something):
        --- Example Dockerfile contents:
            FROM node:slim

            LABEL oscar otenorio90@gmail.com

            WORKDIR /app

            COPY index.js /app/index.js
            COPY package.json /app/package.json
            RUN npm install
            EXPOSE 3000
            ENTRYPOINT node index.js


            You're telling the docker command (which I haven't given yet, this is still part of setup):
                - use a starting template of a node app (slim version so it's smaller)
                - make a LABEL that will note who made/maintains this code
                - create a folder on the docker machine -side called app (where the files will go)
                - grab the contents of the index.js file (note syntax since it's in the same directory as this Dockerfile) and place them into an index.js file you'll create in the app directory
                    NOTE: you can alos just use "COPY . ." to copy all the files in the current directory into a copy in the docker directory
                - do the same for package.json
                - run "npm install" (uses the contents of package.json to download dependencies on the docker machine)
                - open up port 3000
                - use the command "node index.js" to run this code
            NOTE: this applies to any complexity of the app (simple or multi-tiered) but the convention seems to be that you make a Dockerfile for each tier.
                Example:
                √  project-directory
                    √  Client-code
                        >  node_modules
                        >  img
                        √  folder-containing-source-files
                        package.json
                        .gitignore
                        Dockerfile  <-----
                    √  Server-code
                        >  node_modules
                        index.js
                        package.json
                        .gitignore
                        Dockerfile  <-----
                    docker-compose.yml   <----------- NOTE: the yml file should live in the project root where you call the docker build command - it'll go looking into child directories for the Dockerfiles


    - FINALLY actually run the docker build command to dockerize this puppy:
        Run this command from the root directory of the project (where the docker-compose.yml file is):
        - first time running the command use:  docker-compose up
            - creates the image into a container wil default naming conventions based on folder arcchitecture (recommended)
        - following times use:  docker-compose build
            - overwrites the image using the same naming convention (recommended to overwrite your other work). I think "docker-compose up" will fail if it already exists
        


    Docker hub is like the github of docker containers, where you can share your work and others can run it without downloading a bunch of stuff on their machines.

    To add to docker hub:
    - NOTE: first you should go to https://hub.docker.com and create a file. Created my username as "oscartenorio", so wherever you see that in further notes just know that's your username
    - Navigate to root of project that you want to put up into docker hub
    - Run:
        docker build -t oscartenorio/project_name .
        -----a------ -b -----c-----  -----d------ -e
        a. command to create the docker container
        b. adds a tag
        c. corresponds to your docker hub username (must make an account first for this to work)
        d. name of the project (it'll be in the url later)
        e. not sure, but I think it runs the application once built or something?
    - this will upload an image of your project to docker hub, where you can send the URL to anyone (I gugess depending on privacy/permission settings) and have them run your code

    To test that it worked:
        - Run (from the same place in the directory as above, I think):
            docker run -p 8080:3000 --name project_nameInstance -t oscartenorio/project_name
            -----a---- -b -c-- --d- ---e-- --------f----------  ------------g---------------
            a. run command
            b. port tag
            c. port to expose on your machine (D1 local)
            d. port inside of the container
            e. name tag
            f. the name of the container / name of the instanced image (notice the naming convention, so like "exampleInstance" or "myProjectInstance")
            g. destination to run from (same that we created above)
        - To stop it from running:
            - run  docker ps -a  to get a list of the running processes in a NEW terminal, and grab the id of the running container
            - run  docker stop id_you_grabbed  to terminate the process


    To push / pull from Docker hub, you need to log in first:
        docker login -u "myusername" -p "mypassword" docker.io
        - where "myusername" and "mypassword" are replaced with actual username and password (no quotes)

    To push to Docker hub:
        - Run:
            docker push oscartenorio/project_name

    To pull from Docker hub:
    - Run:
        docker pull oscartenorio/project_name

    To remove the container/images:
        - from local machine:
            docker ps -a
            - grab id of the local image
            docker rm id_you_grabbed
        - from docker hub
            docker images
            - grab id of the docker container
            docker rmi id_you_grabbed

- JSON Web Tokens - (JWT) kinda like cookies, these are "keys" that are held client-side that are signed and given out by servers to authenticate a user,
    - though the server does not maintain state (actually remember the user, per se), it has a mechanism of extracting it's secret key from tokens it has handed out to know it's a legit token and thus a legit user
    - it's made up of three parts: header, payload, and signature
        header: contains information about the token itself. It defines the token type and the encryption algorithm it should use
        payload: the most important part of a JWT. It contains information about the client
        signature: a hash that is computed following these steps:
            - Concatenate the encoded header and the encoded payload using a dot
            - Hash the result using the encryption algorithm specified in the header part and a private key
            - Encode the result as a Base64 string

        These are converted from JSON to a base64 string and concatenated using a dot between each part: header.payload.signature

- Swagger - a framework that automatically generates (rather sophisticated) documentation on an application's API.
    You enter some YAML data in comments and it generates a page that clearly communicates how your API works (defining GET syntax, kinds of possible status codes thatt can be received based on your code, etc.)
    Example entry:
        const swaggerDocs = swaggerJsDoc(swaggerOptions);
        app.use('/api-docs', swaggerUI.serve, swaggerUI.setup(swaggerDocs));

        /**                                 <--------- a
        * @swagger                          <-----/
        * /books:                           <----- \
        *   get:                                    \
        *     description: Get all books             \
        *     responses:                              ------ b
        *       200:                                 /
        *         description: Success              /
        *                                          /
        */                                  <-----/
        app.get('/books', (req,res) => {                            <----\
            res.send([                                                    \
                {                                                          \
                    isbn : '9781781100486',                                 \
                    title: 'Harry Potter and the Sorcerer\'s Stone',         \
                    author: 'J.K. Rowling',                                   -------- c  
                    publisher: 'Scholastic'                                  /
                }                                                           /
            ]);                                                            /
        });                                                         <-----/

        a. header that denotes that swagger yaml is being inserted as a comment
        b. uses YAML syntax embedded in the code as a comment, which swaggger will read to generate the documentation page
        c. the uncommented code that actually creates the route being documented. By visiting locahost:whateverPort/api-docs (that path was defined at the top of this example, it can be whatever you want it to be) you can see the generated documentation
        For more on the example provided, see References/Module_25_api_and_holy_grail/api-documentation-and-introduction-to-databases-self-study-activities_starter/Automatic Documentation/app.js

- SDK - Software Developer Kit - A software development kit is a collection of software development tools in one installable package. They facilitate the creation of applications by having a compiler, debugger and sometimes a software framework.
    They are normally specific to a hardware platform and operating system combination

- Event Loop - In Javascript, the Event Loop is a little piece of code that helps facilitate processes that run separate from the current Stack that would be queued onto the Stack once certain criteria is met.
    That criteria is usually defined by an async method that requests an action to be taken outside the normal call stack (like making a request to an api/url, where you're not sure how long it'll take to get a response - since you're communicating with an outside resource)
    A placeholder, or promise, is made stored outside the Stack in the browser's web api that waits for that critereia to be met (think like setTimeout waiting for 5 secs)
    Once the criteria is met, the task gets sent to a task queue -
    the Event Loop is the code that checks for the empty Stack, and once empty grabs a task from the task queue and places it in the Stack to be executed

    Stack - the space where code gets executed. It's single-threaded, meaning it does one task at a time. When given an async function, it passes it to the Web api
    Web Api - where processes are run outside of the task that were assigned from the Stack, like where a setTimeout for 5 secs has it's timer actually running in. Once complete, it passes the task to the Task Queue
    Task Queue - taska are stored here until called upon by the Event Loop
    Event Loop - bit of code that waits for the Stack to be empty, then passes a task from the Task Queue onto the Stack

--------------------------------------------------------------------------------
----------------------------- Copied from Class --------------------------------
--------------------------------------------------------------------------------

- REST (REpresentational State Transfer): copied from a lecture slide:
    Representational state transfer (REST) is a standard for communication between an application’s server and its client. It is considered the industry-standard protocol for web APIs, so you will likely answer questions about REST during software development interviews. It’s important to understand that REST is not a technology or framework; it’s an architectural style to support any API design. 

    RESTful APIs adhere to these five principles.
    1.Contract-first approach / Uniform Resource Identifiers

    The “contract” in the Contract-first approach refers to a contract between your client application and the server. The client app needs to know that it can call the API endpoint and get the data in the expected format each time.

    Contract Example:

    Let’s suppose you had a server with the endpoint “/api/v1/orders/14829/status” that returned the status of order number `14829.` As a RESTful API, the client would expect that:

        The API endpoint does not change
        The data format does not change
        The API documentation describes what information is stored and how

    2. Statelessness

    The client application should assume that the server did not preserve the state from previous API calls. The client application should provide all the information necessary for the server to respond to each call. What does this mean? 

    Say you (the client application) asked your neighbor (the server application) about a mutual friend. In a real-life conversation, the following questions would be stateful:

    How old is Frank Gibson? What does he do for work?

    You (the client application) expect your neighbor (the server application) to remember the mutual friend’s name. So, the second question assumes that the question is about Frank Gibson.

    In a RESTful API, the same conversation between the client and server applications would instead go like this:

    How old is Frank Gibson?

    What does Frank Gibson do for work?

    The client provides all the information a server needs to respond to every API call, assuming that the server remembers nothing of the previous API calls.
    3. Client-server Model

    The client application doesn’t care about how the server stores the data or the technology used. The only thing it cares about is the database schema which contains a description of the stored data and the layout in which it is stored.
    4. Caching

    Caching is the temporary storage of information outside the server. The client can cache data locally to have it available without making an additional API call. 
    5. Layered architecture

    When you have a client application that calls an API endpoint that triggers logic on your back-end server application, you have a layered architecture. What’s important in REST is that each layer only knows about the layer next to it and no more. This is also called a separation-of-concerns. Each layer has a specific job and passes information to and from its neighboring layers to get the job done. Thus, the only layer that the client application knows about is the API, which is its immediate neighbor; it does not know about any other layers beyond the API.

----------------------------------------------------------------
- GraphQL vs REST

    In a standard RESTful API, you typically hit an endpoint that determines what data is returned.GraphQL improves upon the REST philosophy by allowing the front-end application to ask for a specific piece of that data.

    Let's say you want to display the logged-in user's email address on their profile page.

    To do this with a REST API, you would:

        Call an“auth/user” endpoint and get all of the user's information. 
        Pull the email address out of that returned data 
        Display the email address

    Issues with REST:

        If the logged-in user has many data associated with it, such as a list of events, order history, addresses, and phone numbers, it will all be returned even if you don’t need this information. 
        Unnecessary information is passed around. 

    GraphQL allows you to query only the data you want. If you replace the REST endpoint with a GraphQL one, you could query the currently logged-in user but specify you only want their email address. 

----------------------------------------------------------------
- Common packages to hekp a developer in the real world:

    Throughout this course, you’ve seen many packages that you can use to build impressive features and applications. Now let’s look at some that can make your life as a developer easier:

        - Faker.js
        https://www.npmjs.com/package/@faker-js/faker
        As you’ve seen in previous videos, Faker.js can help you generate dummy front-end and back-end data to test your application. You can create people, organizations, accounts, products, images, and more. You can use the same data that are used in production for simplicity and speed. And Faker.js will always keep your data safe.

        - Chalk
        https://www.npmjs.com/package/chalk
        You often check console.log when debugging. However, back-end logs can blend in the terminal and become difficult to identify. With Chalk, you can customize your console.log so that particular errors appear bright red, with a background color, or underlined.

        - Moment
        https://www.npmjs.com/package/moment
        Comparing or manipulating dates or timestamps can be confusing, especially when you have to consider time differences between the user and the server. Moment has been the standard and most widely used date library for a long time, and for a good reason. For instance, if you want to know if one date is older than another, you simply have to write: (png that I can't here) Or maybe you want to add one month to an order creation date to see when it needs to be paid (another png I can't add here).

        Moment does a lot more, and you’re encouraged to take a look at its documentation (https://momentjs.com/) to get a good sense of all you can do with it.

        Note: As of September 2020, Moment recommends using a different date library when building new applications. Nevertheless, with over 12 million downloads per week, Moment is still the most used date library. Therefore, it’s important that you become familiar with its basic functionality and capabilities. For more information and other libraries recommended by Moment, take a look at its Project Status (https://momentjs.com/docs/)

----------------------------------------------------------------
- What Is Heroku?

    https://www.heroku.com/
    Heroku is a cloud service platform that allows you to deploy and run your apps. This platform offers support for a wide range of programming languages, such as Java, Ruby, Node.js, and Python. Heroku runs applications in virtual containers known as Dynos. Think of Dynos as virtual machines that run your applications on the cloud.

    Heroku has a free tier to help you get started. You can create a Heroku account and then follow this getting started guide
    Links to an external site. to start using Heroku services. This will help you accomplish some of the deployment tasks required throughout the course.

----------------------------------------------------------------
- Docker Convenient Notes:
    Useful Docker Commands:

    Build image
    # The -t gives your image a name
    $ docker build -t YourImageName.

    List Containers
    # Containers currently running
    $ docker ps -a

    List Images
    # List all Docker images
    $ docker images -a

    Delete containers
    # Delete every Docker container
    # Run this command first because images are attached to containers
    $ docker rm $(docker ps -a -q)

    Delete images
    # Delete every Docker image
    $ docker rmi $(docker images -q)

    Force delete images
    # to force delete images to prevent their use
    $ docker rmi -f $(docker images -q)

    Stop container
    # Stop a container
    $ docker stop <container id>

    Shell in running container
    # From Docker 1.3 onwards
    $ sudo docker exec -i -t <containerIdOrName> bash
    --------------------------------
    Create and Publish A Docker Image:
    Create an application and a docker file for the application. Then, create a docker image for the application. Name your image YourDockerUsername/YourImageName. In this example the username is "abelsan" and the image name is "cabinfever".

    # Create Docker image
    $ docker build -t abelsan/cabinfever.

    Test your new image.

    # "-p 8080:3000" sets the container port to 3000 and the host port to 8080
    # "--name cabinfeverInstance" sets the container name to cabinfeverInstance
    # "-t cabinfever" references the cabinfever image
    $ docker run -p 8080:3000 --name cabinfeverInstance -t abelsan/cabinfever

    After you confirm your application works, push your image to docker hub.

    # Note the username/image matches the Docker repository
    $ docker push abelsan/cabinfever

    To use the image, enter the pull command.

    $ docker pull abelsan/cabinfever
    --------------------------------
    Docker - NodeJS Hello World:
    Use this document as a way to quickly start any new project.

    Create a barebones NodeJS application
    Create a directory named docnode. Initialize your application (npm init). Install express (npm install express --save). Your package.json file should look like the following:

    {
    "name": "docnode",
    "version": "1.0.0",
    "description": "barebones node on docker",
    "main": "index.js",
    "scripts": {
        "test": "echo \"Error: no test specified\" && exit 1"
    },
    "author": "abel@mit.edu",
    "license": "MIT",
    "dependencies": {
        "express": "^4.15.5"
    }
    }

    Add a bare-bones express server and name your file index.js:
    var express = require('express');
    var app     = express();
    
    app.get('/', function(req,res){
    res.send('Hello World!');
    });
    
    var port = 3000;
    app.listen(port,function(){
    console.log('Listening on port:' + port);
    });

    Create a Docker file and name it Dockerfile:
    FROM node:slim
    MAINTAINER abelsan <abel@mit.edu>
    WORKDIR /app
    # copy code, install npm dependencies
    COPY index.js /app/index.js
    COPY package.json /app/package.json
    RUN npm install

    Create a Docker compose file and name it docker-compose.yml:
    version: "2"
    services:
    gin:
        build: .
        ports:
        - "3000:3000"

    If needed, copy your code to the remote machine. After this is done, at the command line, enter docker-compose up:

    docker-compose up
    --------------------------------
    How To Install Docker On Ubuntu:
    Step 1 — Installing Docker
    The Docker installation package available in the official Ubuntu 16.04 repository may not be the latest version. To get the latest version, install Docker from the official Docker repository. This section shows you how to do that.

    First, add the GPG key for the official Docker repository to the system:

    $ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -

    Add the Docker repository to APT sources:

    $ sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"

    Next, update the package database with the Docker packages from the newly added repository:

    $ sudo apt-get update

    Ensure that you install from the Docker repo instead of the default Ubuntu 16.04 repo:

    apt-cache policy docker-ce

    You should see output similar to the following:

    Output of apt-cache policy docker-ce

    docker-ce:

    Installed: (none)

    Candidate: 17.03.1~ce-0~ubuntu-xenial

    Version table:

    17.03.1~ce-0~ubuntu-xenial 500

    500 https://download.docker.com/linux/ubuntu xenial/stable amd64 Packages

    17.03.0~ce-0~ubuntu-xenial 500

    500 https://download.docker.com/linux/ubuntu xenial/stable amd64 Packages

    Notice that docker-ce is not installed, but the candidate for installation is from the Docker repository for Ubuntu 16.04. Also note that the docker-ce version number might be different.

    Finally, install Docker:

    $ sudo apt-get install -y docker-ce

    Docker should now be installed, the daemon started, and the process enabled to start on boot. Check that it's running:

    $ sudo systemctl status docker

    The output should be similar to the following, showing that the service is active and running:

    Output

    docker.service - Docker Application Container Engine
    Loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset: enabled)

    Active: active (running) since Sun 2016-05-01 06:53:52 CDT; 1 weeks 3 days ago

    Docs: https://docs.docker.com

    Main PID: 749 (docker)

    

    The installation of Docker gives you not just the Docker service (daemon) but also the Docker command line utility or the Docker client. We'll explore how to use the Docker command later in this tutorial.

    Step 2 — Executing The Docker Command Without Sudo (Optional)
    By default, running the Docker command requires root privileges; therefore, you have to prefix the command with sudo. It can also be run by a user in the Docker group, which is automatically created during the Docker Installation. If you attempt to run the Docker command without prefixing it with sudo or without being in the docker group, you'll get an output like this:

    Output

    docker: Cannot connect to the Docker daemon. Is the docker daemon running on this host?.

    See 'docker run --help'.

    If you want to avoid typing sudo whenever you run the Docker command, add your username to the Docker group:

    $ sudo usermod -aG docker ${USER}

    To apply the new group membership, log out of the server, then log back in.
    ----------------------------------------------------------------


----------------------------------------------------------------
- Why Use JSON Web Tokens?

    JSON Web Tokens (JWTs) provide a better alternative than session IDs to solving the client–server communication problem. 

    They define a way to transmit information related to authentication and authorization between two parties. 
    Each token issued is digitally signed and contains all the information for the server to allow or deny a given request. This means that the server does not have to access a database to verify a request, which would expose the application to security threats.
    Tokens are stored in the client, so they can be verified by the server without accessing a database. This reduces the security threat of multiple database queries and also reduces the time it would take for the token to be verified. 
    JSON Web Tokens provide a secure and efficient solution to the client-server communication problem.

----------------------------------------------------------------

What is API Documentation?
    API documentation contains clear and concise instructions on how to use and integrate an API. 

    API Documentation Best Practices
    Write Documentation for Nontechnical People
    All users will have different levels of experience and knowledge, so it is important that you write documentation in simple, clear language.

    Review the documentation for Twilio’s APIsLinks to an external site., for instance. The writers describe and define what an API is and how it works at the very beginning of the guide. They don’t assume that everyone reading this documentation is an expert.

    Include Explicit Request and Response Cycles
    Those using your API need to know what your endpoint expects within the request body and what it could return as a response. For example, if your endpoint runs into an error, what response status and message will be returned? Detailing that in the documentation will help users understand what to expect and what’s needed.

    Keep It as Simple as Possible
    A user trying to read through three paragraphs of text on one single endpoint will feel frustrated. Where possible, keep your documentation simple and short, without sacrificing readability.

    Keep It Up-to-Date
    It is important to maintain your documentation. Out-of-date API documentation can lead a developer in the wrong direction and waste their time.

----------------------------------------------------------------

- Asynchronous Callbacks
    With an asynchronous callback like setTimeout, for example, you can make JavaScript wait to fire a function for five seconds while it runs another block of code in the meantime, as shown below:

    console.log("Hello");
    setTimeout(function() {
    console.log("world!");
    }, 5000);

    console.log("MITxPRO Coding Course!")

    When you run this, it will show up in your console as follows:

    Hello
    MITxPRO Coding Course 
    //5 seconds later
    world!
    If you use setTimeout, your code can become complicated. You could end up with nested setTimeout functions inside of each other as you try to execute blocks of code in a timed sequence. For example, suppose you have a basic shape that you want to animate, as in the following sequence:

    Change the inner text into “Let’s go!” after 500 ms
    Turn the square into a diamond after 2000 ms
    Turn the diamond green after 500 ms
    Turn the diamond into a square after 250 ms
    Turn the square purple after 300 ms
    Fade the entire shape out after 500 ms

    The code would look something like this: 

    <script>
    const go = document.querySelector('.go');
    go.addEventListener('click', function(e) {
        const el = e.currentTarget;
        el.textContent = "LET’S GO!"
        setTimeout(function() {
        el.classList.add('diamond');
        setTimeout(function() {
            el.classList.add('green');
            setTimeout(function(){
            el.classList.remove('circle');
            el.classList.add(‘square’);
            setTimeout(function(){
                el.classList.remove('green');
                el.classList.add('purple');
                setTimeout(function() {
                el.classList.add('fadeout');
                }, 500)
            }, 300)
            }, 250)
        }, 500)
        }, 2000)
    });
    
    </script>
    

    As you can see, this is inefficient and can lead to confusion. This is where promises can help.

    Promises
    Promises help you write asynchronous JavaScript in a concise manner. A promise “promises” to return a response for something that will happen in the future, which is how it got its name.

    Suppose you request data from an external API. You would not receive the data immediately; instead, you would receive a promise from the API, which is like a ticket that says, “Okay, your request  has been received, and you’ll receive that data soon.” A real-world example of a promise is going to a restaurant, ordering food, and then receiving a ticket with a number that the staff will call when your food is ready.

    To illustrate how promises work, let’s imagine that you’re at a sandwich shop. When you write a promise, you’ll create the code for the request and the response. 

    function makeSandwich() {

        const sandwichPromise = new Promise();

        return sandwichPromise;

    }
    The promise is not fulfilled or resolved until the response is ready (or, in this case, until the sandwich is done). The promise request is made immediately, but the response is resolved later.

    Resolve And Reject Functions

    A promise will give you a resolve and a reject function. Call the resolve function fulfills the promise and returns whatever value you passed to it. Call the reject function when there is something unexpected or an error in your promise that you want to return. 

    function makeSandwich(fillings) {

        const sandwichPromise = new Promise(function(resolve, reject){

        //when you're ready, you can resolve the promise

        resolve(`here is your sub! Your fillings are ${toppings.join(' ')}`)

        //if something went wrong, you can reject this promise
        reject({error:’something went wrong’})

        });

        return sandwichPromise;

    }
    .Then

    Now, how do you access what you resolved inside of the promise? (In this case, the sandwich object). You need to add .then to the promise chain in order to return the object that resolved the promise. It will look like this:

    function makeSandwich(fillings) {

        const sandwichPromise = new Promise(function(resolve, reject){

        //when you're ready, you can resolve the promise

        resolve(`here is your sub! Your fillings are ${fillings.join(' ')}`)

        //if something went wrong, you can reject this promise

        });

        return sandwichPromise;

    }

    const italianSandwichPromise = makeSandwich(['salami', 'provolone', 'ham']);

    const tunaSandwichPromise = makeSandwich(['tuna', 'lettuce', 'tomato']);

    italianSandwichPromise.then(function(sandwich){

        console.log(sandwich)

    })
    Now, if you were to drag and drop that code into your console (with the appropriate HTML and script tags), you would see this:

    here is your sandwich! Your fillings are salami provolone ham

    Why is this more useful? 

    If you want to make multiple API calls to multiple sources, you can do so by chaining multiple promises together in a row with .then statements only one level deep, as illustrated with the sandwich example below. This is more efficient than using nested callbacks.

    makeSandwich(['tuna'])

    .then(function(sandwich){

        console.log(sandwich);

        return makeSandwich(['meatballs']);

    })

    .then(function(sandwich){

        console.log(sandwich);

        return makeSandwich(['roast beef', 'swiss', 'mayo']);

    }).then(function(sandwich){

        console.log(sandwich)

    })
    

    In your console, you’ll see the output of the .then statements:

        here is your sub! Your fillings are salami provolone ham

        here is your sub! Your fillings are meatballs

        here is your sub! Your fillings are roast beef swiss mayo

----------------------------------------------------------------

- Firebase App Pattern

    Before writing a Firebase application, it’s important to review the pattern of the application architecture. You’ll need to reference the Firebase library on the main HTML page,
    as well as referencing the relevant Firebase libraries for each of the specific features you intend to use. To see a list of firebase libraries that you may need to add to your application, review the Firebase documentation (https://firebase.google.com/docs/web/setup#available-libraries).
    For example, if you need to use firebase authentication, then you would add the corresponding script tag, listed in the documentation, to your application’s HTML page.

    Core elements needed to use Firebase:
    1. Create an account at firebase.google.com
    2. Script tags in HTML page:
        - General library
        - libraries for the speicifc functionality you need (authentication, database, etc)
        - JS file hosting other code needed for above libraries to function (i.e. initializing config, database, handles, etc)

----------------------------------------------------------------

- Cloud Functions
    Cloud functions (https://cloud.google.com/functions) allow you to write and group functions and deploy them for use across different applications. In this video, you’ll deploy a function via Firebase.
    You could then incorporate this function into another application, without worrying about maintaining it as you would if you had to manage a traditional server.

    Note: While Firebase functions offer a free usage tier, Firebase requires that you upgrade your account billing to the “Blaze plan” which will require a payment method.
    It is unlikely that you will exceed the Firebase free usage tier. Moreover, this is an optional activity and you are not required to use cloud functions.

    To use cloud functions, you can:

    - Start by creating a directory in your terminal window and installing firebase tools. 
        - Note that, in the video, Dr. Sanchez uses the sudo npm install -g firebase-tools command. This command will only work for Mac users. If you use Windows or Linux, you can write npm install -g firebase-tools.
    - Log in (you will need to authenticate with a Google account).
    - Initialize Firebase.
    - Select functions.
    - Use an existing project: in this case, courso.
    - Select JavaScript and say no to linting.
    - Install dependencies for cloud functions.
    - Open your index.js file, uncomment the code, and save it.
    - Deploy to the cloud.

----------------------------------------------------------------


- Cloud Services vs. Microservices
    Cloud services can often be confused with the term microservices. Cloud services are infrastructure, platforms, or software that are hosted by third-party providers, such as Google, Amazon, and Microsoft. These services can be used to host your application or database, send emails and sms messages, build mobile applications, handle user authentication, and much more.

    Microservices describes a specific architecture that includes single-function modules (hence, “micro”). These single-function modules have well-defined interfaces and operations. You would likely use cloud services to host these single-function modules; in fact, different modules may be hosted by different cloud services. It’s important to remember that microservices describe an architectural decision, not a technology.

    Cloud Service Providers
    This week, you learned about Firebase, which allows you to use multiple Google Cloud services in an easy-to-use interface. There are multiple cloud service providers to choose from, and most offer very similar services but with different naming conventions and pricing strategies. This can make it confusing when trying to find a solution for your application. While there are many Cloud Service Providers, this discussion will focus on the big three: Google Cloud Platform (GCP), Amazon Web Services (AWS), and Microsoft Azure. Let’s look at some of the popular services across all three providers.

    Cloud Storage
    Cloud storage can be used to store files, as well as host single-page web applications. All three cloud providers charge based on storage and the amount of bandwidth used to read and write the data.

    Google Cloud  |	AWS   |	 Microsoft Azure
    Cloud Storage	|   S3   |	Azure Blob Storage


    Cloud Functions
    Cloud functions allow you to run your code without server management and automatically scale up or down depending on the use. You only pay for usage, so if your cloud functions are not being used, then you’re not incurring any charges.

    Google Cloud  |	AWS  |	Microsoft Azure
    Firebase/Cloud Functions  |	AWS Lambda	|  Azure Functions


    Virtual Machines
    Virtual machines (VMs) are full-fledged servers that you can customize. With that flexibility comes the cost of maintaining them. All three providers charge based on the size of the VM, but also offer free credits so you can try them before paying for the service.

    Google Cloud  |	AWS  |	Microsoft Azure
    Compute Engine   |	Elastic Compute Cloud (EC2)  |	Azure Virtual Machines
    These are just some of the services provided by these three cloud providers

----------------------------------------------------------------

- Relational VS Non-Relational Databases

    Relational Databases
    A relational database stores data in a structure of multiple tables; each table is called an entity. The relationships between entities are structured and well defined, but not very flexible.

    Relational databases are also called SQL databases. SQL stands for Structured Query Language, and it’s the language in which relational databases are written. SQL is used to execute queries, retrieve data, and edit data by updating, deleting, or creating new records.

    Popular SQL databases include: 
    - SQL Server
    - MySQL
    - PostgreSQL
    - Advantages Of Relational Databases

    Relational databases offer features that remain critical today as developers build the next generation of applications: 
    - Indexed data rows and a query language
        SQL allows you to easily query data and build well-defined views of it. You can access and manipulate data by using indexes or row ids. 
    - Consistency
        Relational databases are structured to promote greater data consistency. This allows applications to immediately read what has been written to the database. 
    - Enterprise Management and Integration
        Organizations need a database that can be secured, monitored, automated, and integrated with their existing technology infrastructure, processes, and staff. Relational databases fit into the enterprise application infrastructure easily.

    Disadvantages of Relational Database
    - Flexibility
        Relational databases require data to be structured within entity-relationship diagrams. Relational databases cannot accommodate unstructured data, which is often produced by modern applications. 
    - Scalability
        Relational databases often experience latency issues when accomodating large amounts of data.


    Non-relational Databases
    A non-relational database does not use tables and columns to store data. Instead, its storage model is optimized for the type of data it’s storing. Non-relational databases are also known as NoSQL databases, which stands for Not Only SQL. Whereas relational databases only use SQL, non-relational databases can use other types of query languages.

    There are four different types of NoSQL databases: 
    - Document-oriented databases
    - Key-value stores 
    - Wide-column stores 
    - Graph stores 
    This course focuses on document-oriented databases, also known as document stores. Document databases pair each key with a complex data structure called a document. 

    Popular document-oriented databases include:
    - MongoDB
    - RedisDB

    Advantages Of Non-relational Databases

    Modern applications impose requirements that are not addressed by relational databases. This has driven the development of NoSQL databases that offer the following:
    - A flexible data model
        NoSQL databases offer flexible data models that make it easy to store and combine data in any structure and allow the dynamic modification of the data structure without impacting the application. 
    - Scalability and Performance
        NoSQL databases were all built with a focus on scalability. This allows the database to scale out on hardware deployed on-premises or in the cloud and thus enables almost unlimited growth with higher throughput and lower latency than that offered by relational databases.

    Disadvantages of Non-relational Databases:
    - Structure and Consistency
        Because non-relational databases don’t require a predefined data structure, they are not well suited for use cases that require consistent, structured data.  
    - Enterprise applications
        Legacy enterprise systems and teams are often to expect a specific data structure from a relational database. It would be difficult to adapt these systems to rely on a non-relational database. 

    What Type of Database Should You Use?

    Ultimately, the choice of database depends on the data structure, data size, and how the data will be used and analyzed. However, a recommended starting point is to answer these questions:
    - Which type of data will you be storing?  Does your data fit into rows and columns, or is it better suited to a more flexible structure?  
    - How much data are you handling? The bigger the data set, the more likely a non-relational database is a better fit. Non-relational databases can store large amounts of data and give you the flexibility to change the data type at any point without breaking the database model.
    
    In this course, you’ll be focusing on NoSQL databases, as these offer the flexibility needed when developing MERN applications.

----------------------------------------------------------------


-------------------------------------------------------------------------------------------------
--------------------- List of Programs/Langauges/Frameworks used so far -------------------------
-------------------------------------------------------------------------------------------------

- JavaScript - language
- React - Javascript library to have reactive apps
- Node JS - javascript runtime environment
- Express - backend framework built on Javascript
- Postman - API platform for testing APIs
- MongoDB - database
- Strapi - lightweight database
- Docker - essentially Virtual Machine service packaged into containers
- Kubernetes - cluster of Docker containers
- Nodemon - auto-refreshes apps once changes saved
- Swaggger - auto-generates API documentation
- GraphQL - interface to query databases
- LowDB - small JSON database for Node
- Babel - makes newer Javascript backwards compatible (for older browsers)
- Superagent - http request library for node.js
- Redis - database
- Firebase - google backend computing service to host databses, services, authentication, etc (ties to youyr gmail account)
- Maven - automated build tool primarily used for Java, C#, Ruby, etc (not used in class but mentioned)

--------------------------------------------------------------------------------
------------------------------- Useful Links -----------------------------------
--------------------------------------------------------------------------------

- https://reactjs.org/ - React JS documentation and resources

- https://getbootstrap.com - Bootstrap CSS documentation and resources

- https://strapi.io/ - Strapi (back-end database) documentation and resources

- https://www.npmjs.com/ - npm package registry (look up commands to install specific packages)
